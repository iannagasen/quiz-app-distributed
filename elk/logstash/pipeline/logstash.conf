# input {
#   file {
#     # TODO: in dockerized applications, this should be in a volume or other.
#     # maybe /root/temp/elk-logs
#     path => "/root/elk-logs/elk-stack.log"
#     start_position => "beginning"
#   }
# }

# output {
#   stdout {
#     codec => rubydebug
#   }

#   # sending properly parse log events to elasticsearch
#   elasticsearch {
#     ## elasticsearch:9200 is for dockerize elasticsearch, if not, use localhost:9200
#     hosts => ["elasticsearch:9200"]
#   }
# }

input {
  tcp {
    port => 5000
    codec => json
  }
  file {
    path => "/usr/share/logstash/pipeline/products.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => plain {
      charset => "ISO-8859-1"
    }
  }
}

filter {
  csv {
    separator => ","
    columns => ["id","title","description","manufacturer","price"]
  }

  mutate {
    remove_field => ["@version","@timestamp","path","host", "tags", "message"]
  } 
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    # index => "springboot-%{app}"
    index => "springboot-app-index"
  }

  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "amazon_products"
  }

  stdout {}
}